{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "43. Deep Mind Matthew",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/exoticspaces/conda-docs/blob/master/43_Deep_Mind_Matthew.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "G_V6GMX775jZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np,sys,os\n",
        "from sklearn.utils import shuffle\n",
        "from scipy.ndimage import imread\n",
        "from scipy.misc import imresize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(678)\n",
        "tf.set_random_seed(5678)\n",
        "\n",
        "def tf_relu(x): return tf.nn.relu(x)\n",
        "def d_tf_relu(s): return tf.cast(tf.greater(s,0),dtype=tf.float32)\n",
        "\n",
        "def tf_tanh(x): return tf.nn.tanh(x)\n",
        "def d_tf_tanh(x): return 1-tf_tanh(x) ** 2\n",
        "\n",
        "def tf_softmax(x): return tf.nn.softmax(x)\n",
        "\n",
        "# --- make class ---\n",
        "class CNN_Layer():\n",
        "    \n",
        "    def __init__(self,ker,in_c,out_c):\n",
        "        self.w = tf.Variable(tf.random_normal([ker,ker,in_c,out_c],stddev=0.005))\n",
        "\n",
        "    def feedforward(self,input,stride=1,dilate=1):\n",
        "        self.input  = input\n",
        "        self.layer  = tf.nn.conv2d(input,self.w,strides = [1,stride,stride,1],padding='SAME')\n",
        "        self.layerA = tf_relu(self.layer)\n",
        "        return self.layerA\n",
        "    \n",
        "    def backprop(self,gradient):\n",
        "        \n",
        "        grad_part_1 = gradient \n",
        "        grad_part_2 = d_tf_relu(self.layer)\n",
        "        grad_part_3 = self.input\n",
        "\n",
        "        grad_middle = tf.multiply(grad_part_1,grad_part_2)\n",
        "        grad = tf.nn.conv2d_backprop_filter(\n",
        "            input = grad_part_3,\n",
        "            filter_sizes = self.w.shape,\n",
        "            out_backprop = grad_middle,\n",
        "            strides=[1,1,1,1],padding='SAME'\n",
        "        )\n",
        "\n",
        "        grad_pass = tf.nn.conv2d_backprop_input(\n",
        "            input_sizes=[batch_size] + list(self.input.shape[1:]),\n",
        "            filter = self.w,\n",
        "            out_backprop = grad_middle,\n",
        "            strides=[1,1,1,1],padding='SAME'\n",
        "        )\n",
        "\n",
        "        update_w = []\n",
        "        update_w.append(tf.assign(self.w, self.w - learning_rate * grad))\n",
        "        return grad_pass,update_w\n",
        "\n",
        "class FNN_layer():\n",
        "    def __init__(self,input_dim,hidden_dim):\n",
        "        self.w = tf.Variable(tf.truncated_normal([input_dim,hidden_dim], stddev=0.005))\n",
        "\n",
        "    def feedforward(self,input=None):\n",
        "        self.input = input\n",
        "        self.layer = tf.matmul(input,self.w)\n",
        "        self.layerA = tf_tanh(self.layer)\n",
        "        return self.layerA\n",
        "\n",
        "    def backprop(self,gradient=None):\n",
        "        grad_part_1 = gradient\n",
        "        grad_part_2 = self.d_act(self.layer)\n",
        "        grad_part_3 = self.input \n",
        "\n",
        "        grad_x_mid = tf.multiply(grad_part_1,grad_part_2)\n",
        "        grad = tf.matmul(tf.transpose(grad_part_3),grad_x_mid)\n",
        "        grad_pass = tf.matmul(tf.multiply(grad_part_1,grad_part_2),tf.transpose(self.w))\n",
        "\n",
        "        update_w = []\n",
        "        update_w.append(tf.assign(self.w, self.w - learning_rate * grad))\n",
        "        return grad_pass,update_w\n",
        "\n",
        "# --- get data ---\n",
        "data_location = \"./DRIVE/training/images/\"\n",
        "train_data = []  # create an empty list\n",
        "for dirName, subdirList, fileList in sorted(os.walk(data_location)):\n",
        "    for filename in fileList:\n",
        "        if \".tif\" in filename.lower():  # check whether the file's DICOM\n",
        "            train_data.append(os.path.join(dirName,filename))\n",
        "\n",
        "data_location = \"./DRIVE/training/1st_manual/\"\n",
        "train_data_gt = []  # create an empty list\n",
        "for dirName, subdirList, fileList in sorted(os.walk(data_location)):\n",
        "    for filename in fileList:\n",
        "        if \".tif\" in filename.lower():  # check whether the file's DICOM\n",
        "            train_data_gt.append(os.path.join(dirName,filename))\n",
        "\n",
        "\n",
        "train_images = np.zeros(shape=(128,64,64,1))\n",
        "train_labels = np.zeros(shape=(128,64,64,1))\n",
        "\n",
        "for file_index in range(len(train_data)):\n",
        "    train_images[file_index,:,:]   = np.expand_dims(imresize(imread(train_data[file_index],mode='F',flatten=True),(64,64)),axis=2)\n",
        "    train_labels[file_index,:,:]   = np.expand_dims(imresize(imread(train_data_gt[file_index],mode='F',flatten=True),(64,64)),axis=2)\n",
        "\n",
        "train_images = (train_images - train_images.min()) / (train_images.max() - train_images.min())\n",
        "train_labels = (train_labels - train_labels.min()) / (train_labels.max() - train_labels.min())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- hyper ---\n",
        "num_epoch = 100\n",
        "init_lr = 0.01\n",
        "batch_size = 2\n",
        "\n",
        "# --- make layer ---\n",
        "l1 = CNN_Layer(5,1,10)\n",
        "l2 = CNN_Layer(5,10,25)\n",
        "\n",
        "FNN_Input = 64 * 64 * 25 \n",
        "l3 = FNN_layer(FNN_Input,1000)\n",
        "l4 = FNN_layer(1000,64*64)\n",
        "\n",
        "# ---- make graph ----\n",
        "x = tf.placeholder(shape=[None,64,64,1],dtype=tf.float32)\n",
        "y = tf.placeholder(shape=[None,64*64],dtype=tf.float32)\n",
        "\n",
        "layer1 = l1.feedforward(x)\n",
        "layer2 = l2.feedforward(layer1)\n",
        "layer3_Input = tf.reshape(layer2,[batch_size,-1])\n",
        "layer3 = l3.feedforward(layer3_Input)\n",
        "layer4 = l4.feedforward(layer3)\n",
        "\n",
        "final_softmax = tf_softmax(layer4)\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=layer4,labels=y))\n",
        "auto_train = tf.train.GradientDescentOptimizer(learning_rate=init_lr).minimize(cost)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- start session ---\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for iter in range(num_epoch):\n",
        "        \n",
        "        # train\n",
        "        for current_batch_index in range(0,len(train_images),batch_size):\n",
        "            current_batch = train_images[current_batch_index:current_batch_index+batch_size,:,:,:]\n",
        "            current_label = np.reshape(train_labels[current_batch_index:current_batch_index+batch_size,:,:,:],(batch_size,-1))\n",
        "            sess_results = sess.run([cost,auto_train],feed_dict={x:current_batch,y:current_label})\n",
        "            print(' Iter: ', iter, \" Cost:  %.32f\"% sess_results[0],end='\\r')\n",
        "\n",
        "        print('\\n-----------------------')\n",
        "        train_images,train_labels = shuffle(train_images,train_labels)\n",
        "\n",
        "        if iter % 2 == 0:\n",
        "            test_example =   train_images[:batch_size,:,:,:]\n",
        "            test_example_gt = np.reshape(train_labels[:batch_size,:,:,:],(batch_size,-1))\n",
        "            sess_results = sess.run([final_softmax],feed_dict={x:test_example})\n",
        "\n",
        "            sess_results = np.reshape(sess_results[0][0],(64,64))\n",
        "            test_example = test_example[0,:,:,:]\n",
        "            test_example_gt =np.reshape( test_example_gt[0],(64,64))\n",
        "\n",
        "            plt.figure()\n",
        "            plt.imshow(np.squeeze(test_example),cmap='gray')\n",
        "            plt.axis('off')\n",
        "            plt.title('epoch_'+str(iter)+'Original Image')\n",
        "            plt.savefig('train_change/epoch_'+str(iter)+\"a_Original_Image.png\")\n",
        "\n",
        "            plt.figure()\n",
        "            plt.imshow(np.squeeze(test_example_gt),cmap='gray')\n",
        "            plt.axis('off')\n",
        "            plt.title('epoch_'+str(iter)+'Ground Truth Mask')\n",
        "            plt.savefig('train_change/epoch_'+str(iter)+\"b_Original_Mask.png\")\n",
        "\n",
        "            plt.figure()\n",
        "            plt.imshow(np.squeeze(sess_results),cmap='gray')\n",
        "            plt.axis('off')\n",
        "            plt.title('epoch_'+str(iter)+'Generated Mask')\n",
        "            plt.savefig('train_change/epoch_'+str(iter)+\"c_Generated_Mask.png\")\n",
        "\n",
        "            plt.figure()\n",
        "            plt.imshow(np.multiply(np.squeeze(test_example),np.squeeze(test_example_gt)),cmap='gray')\n",
        "            plt.axis('off')\n",
        "            plt.title('epoch_'+str(iter)+\"Ground Truth Overlay\")\n",
        "            plt.savefig('train_change/epoch_'+str(iter)+\"d_Original_Image_Overlay.png\")\n",
        "\n",
        "            plt.figure()\n",
        "            plt.axis('off')\n",
        "            plt.imshow(np.multiply(np.squeeze(test_example),np.squeeze(sess_results)),cmap='gray')\n",
        "            plt.title('epoch_'+str(iter)+\"Generated Overlay\")\n",
        "            plt.savefig('train_change/epoch_'+str(iter)+\"e_Generated_Image_Overlay.png\")\n",
        "\n",
        "            plt.close('all')\n",
        "\n",
        "        # save image if it is last epoch\n",
        "        if iter == num_epoch - 1:\n",
        "            train_images,train_labels = shuffle(train_images,train_labels)\n",
        "            \n",
        "            for current_batch_index in range(0,len(train_images),batch_size):\n",
        "                current_batch = train_images[current_batch_index:current_batch_index+batch_size,:,:,:]\n",
        "                current_label = np.reshape(train_labels[current_batch_index:current_batch_index+batch_size,:,:,:],(batch_size,-1))\n",
        "                sess_results = sess.run([cost,auto_train,final_softmax],feed_dict={x:current_batch,y:current_label})\n",
        "\n",
        "                plt.figure()\n",
        "                plt.imshow(np.squeeze(current_batch[0,:,:,:]),cmap='gray')\n",
        "                plt.axis('off')\n",
        "                plt.title(str(current_batch_index)+\"a_Original Image\")\n",
        "                plt.savefig('gif/'+str(current_batch_index)+\"a_Original_Image.png\")\n",
        "\n",
        "                plt.figure()\n",
        "                plt.imshow(np.reshape(np.squeeze(current_label[0,:,:,:],(64,64) ) ),cmap='gray')\n",
        "                plt.axis('off')\n",
        "                plt.title(str(current_batch_index)+\"b_Original Mask\")\n",
        "                plt.savefig('gif/'+str(current_batch_index)+\"b_Original_Mask.png\")\n",
        "                \n",
        "                plt.figure()\n",
        "                plt.imshow(np.squeeze(np.reshape(sess_results[2][0]),(64,64) ),cmap='gray')\n",
        "                plt.axis('off')\n",
        "                plt.title(str(current_batch_index)+\"c_Generated Mask\")\n",
        "                plt.savefig('gif/'+str(current_batch_index)+\"c_Generated_Mask.png\")\n",
        "\n",
        "                plt.figure()\n",
        "                plt.imshow(np.multiply(np.squeeze(current_batch[0,:,:,:]),np.squeeze(np.reshape(current_label[0](64,64) ) )),cmap='gray')\n",
        "                plt.axis('off')\n",
        "                plt.title(str(current_batch_index)+\"d_Original Image Overlay\")\n",
        "                plt.savefig('gif/'+str(current_batch_index)+\"d_Original_Image_Overlay.png\")\n",
        "            \n",
        "                plt.figure()\n",
        "                plt.imshow(np.multiply(np.squeeze(current_batch[0,:,:,:]),np.squeeze(np.reshape(sess_results[2][0](64,64) ))),cmap='gray')\n",
        "                plt.axis('off')\n",
        "                plt.title(str(current_batch_index)+\"e_Generated Image Overlay\")\n",
        "                plt.savefig('gif/'+str(current_batch_index)+\"e_Generated_Image_Overlay.png\")\n",
        "\n",
        "                plt.close('all')\n",
        "\n",
        "\n",
        "# -- end code --"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}